{
 "metadata": {
  "name": "",
  "signature": "sha256:4c513787d977f064a2bbec1a7d637fdaba015e5265450dd9f494f9c2a41e89ff"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import pylab\n",
      "limits=pylab.axis('off')\n",
      "#DATADIR = \"/home/brian/UofU/Radiology/NLP\"\n",
      "DATADIR = \"/Users/brian/Dropbox/NLP\"\n",
      "DATADIR2 = \"/Users/brian/Documents/Radiology/NLP\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "outputs": [],
     "source": [
      "Licensing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "[http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "outputs": [],
     "source": [
      "Program Description"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "processReports.py is a program that uses pyConTextNLP to process reports\n",
      "in a Microsfot Access database.\n",
      "\n",
      "By default the database has a table named 'test_MyReports' with a field named 'report'.\n",
      "These values can be be specified through the command line options.\n",
      "\n",
      "The code is currently written to support either a Microsoft Access database or a SQLite database.\n",
      "\n",
      "Much of the class constructor is dedicated to creating tables in the database for writing the results.\n",
      "A number of tables are created including a table containing the schema used, the arguments specified\n",
      "on the command line and the results.\n",
      "\n",
      "The heart of the code is in analyzeReport where the pyConTextNLP algorithm is applied to an individual report."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pyConTextNLP\n",
      "import pyConTextNLP.pyConTextGraph as pyConText\n",
      "import pyConTextNLP.itemData as itemData\n",
      "import pyConTextNLP.helpers as helpers\n",
      "import getpass\n",
      "import time\n",
      "import xml.dom.minidom as minidom\n",
      "import sqlite3 as sqlite\n",
      "import os\n",
      "import sys\n",
      "import networkx as nx\n",
      "from collections import Counter\n",
      "#import ast\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def instantiateSchema(values,rule):\n",
      "    \"\"\"evaluates rule by substituting values into rule and evaluating the resulting literal.\n",
      "    For security the ast.literal_eval() method is used.\n",
      "    \"\"\"\n",
      "    r = rule\n",
      "    for k in values.keys():\n",
      "        r = r.replace(k,values[k].__str__())\n",
      "    #return ast.literal_eval(r)\n",
      "    return eval(r)\n",
      "\n",
      "def assignSchema(values,rules):\n",
      "    for k in rules.keys():\n",
      "        if( instantiateSchema(values,rules[k][1]) ):\n",
      "            return k\n",
      "\n",
      "def modifies(g,n,modifiers):\n",
      "    \"\"\"Tests whether any of the modifiers of node n are in any of the categories listed in 'modifiers'\"\"\"\n",
      "    pred = g.predecessors(n)\n",
      "    if( not pred ):\n",
      "        return False\n",
      "    #pcats = [p.getCategory().lower() for p in pred]\n",
      "    pcats = []\n",
      "    for p in pred:\n",
      "        pcats.extend(p.getCategory())\n",
      "    return bool(set(pcats).intersection([m.lower() for m in modifiers]))\n",
      "def matchedModifiers(g,n,modifiers):\n",
      "    \"\"\"returns the set of predecessors of node 'n' that are of a category contained in 'modifiers'\"\"\"\n",
      "    pred = g.predecessors(n)\n",
      "    #print n,pred\n",
      "    if( not pred ):\n",
      "        return False\n",
      "    #print \"in matchedModifiers\"\n",
      "    #pcats = [p.getCategory().lower() for p in pred]\n",
      "    pcats = []\n",
      "    for p in pred:\n",
      "        pcats.extend(p.getCategory())\n",
      "    #print pcats\n",
      "    #print [m.lower() for m in modifiers]\n",
      "    #print set(pcats).intersection([m.lower() for m in modifiers])\n",
      "    return set(pcats).intersection([m.lower() for m in modifiers])\n",
      "\n",
      "def returnMatchedModifiers(g,n,modifiers):\n",
      "    pred = g.predecessors(n)\n",
      "    if( not pred ):\n",
      "        return []\n",
      "    mods = [m.lower() for m in modifiers]\n",
      "    mmods = [p for p in pred if p.isA(mods)]\n",
      "    return mmods\n",
      "def getSeverity(g,t,severityRule):\n",
      "    if(not t.isA(severityRule[0]) ):\n",
      "        return []\n",
      "    smods = returnMatchedModifiers(g,t,severityRule[1])\n",
      "    if( smods ):\n",
      "        severityResults = []\n",
      "        for m in smods:\n",
      "            mgd = m.getMatchedGroupDictionary()\n",
      "            val = mgd.get('value')\n",
      "            units = mgd.get('unit')\n",
      "            phrase = m.getPhrase()\n",
      "        severityResults.append((phrase,val,units))\n",
      "        return severityResults\n",
      "    else:\n",
      "        return []\n",
      "def anatomyRecategorize(g,t,categoryRule):\n",
      "    \"\"\"create a new category based on categoryRule\"\"\"\n",
      "    if( not t.isA( categoryRule[0] ) ):\n",
      "        return \n",
      "    mods = g.predecessors(t)\n",
      "\n",
      "    if( mods ):\n",
      "        \n",
      "        mmods = matchedModifiers(g,t,categoryRule[1])\n",
      "        if( mmods ):\n",
      "            newCategory = []\n",
      "            for m in mmods:\n",
      "                nc = \"_\".join([m.lower(),categoryRule[0]])\n",
      "                newCategory.append(nc)\n",
      "            t.replaceCategory(categoryRule[0],newCategory)\n",
      "def genericClassifier(g,t,rule):\n",
      "    \"\"\"based on the modifiers of the target 't' and the provide rule in 'rule' classify the target node\"\"\"\n",
      "    mods = g.predecessors(t)\n",
      "    if( not mods ):\n",
      "        return rule[\"DEFAULT\"]\n",
      "    for r in rule[\"RULES\"]:\n",
      "        if( modifies(g,t,r[1]) ):\n",
      "            return r[0]\n",
      "    return rule[\"DEFAULT\"]\n",
      "\n",
      "\n",
      "# In[6]:\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class criticalFinder(object):\n",
      "    \"\"\"This is the class definition that will contain the majority of processing\n",
      "    algorithms for criticalFinder.\n",
      "\n",
      "    The constructor takes as an argument the name of an SQLite/Access database containing\n",
      "    the relevant information.\n",
      "    \"\"\"\n",
      "    def __init__(self, schema='',rules='',rid='',column='',table='',result_label='',mode='sqlite',dbname='',\n",
      "                 lexical_kb = None, domain_kb = None, debug=False):\n",
      "        \"\"\"create an instance of a criticalFinder object associated with the SQLite\n",
      "        database.\n",
      "        dbname: name of SQLite database\n",
      "        \"\"\"\n",
      "        if( lexical_kb == None ):\n",
      "            lexical_kb = []\n",
      "        if( domain_kb == None ):\n",
      "            domain_kb = []\n",
      "\n",
      "        self.lexical_kb = lexical_kb\n",
      "        self.domain_kb = domain_kb\n",
      "        if( not schema ):\n",
      "            print \"You must specify a schema file\"\n",
      "            sys.exit(0)\n",
      "        try:\n",
      "            self.readSchema(schema)\n",
      "        except Exception, error:\n",
      "            print \"The schema is not in the correct format\",error\n",
      "            sys.exit(0)\n",
      "        if( not rules ):\n",
      "            print \"you must specify a rules file\"\n",
      "            sys.exit(0)\n",
      "        try:\n",
      "            self.readRules(rules)\n",
      "        except Exception, error:\n",
      "            print \"The rules are not in the correct format\", error\n",
      "            sys.exit(0)\n",
      "        # Define queries to select data from the SQLite database\n",
      "        # this gets the reports we will process\n",
      "        self.rid = rid\n",
      "        self.column = column\n",
      "        self.table = table\n",
      "        self.result_label = result_label\n",
      "        self.query1 = '''SELECT %s,%s FROM %s WHERE month = \"OCT\"'''%(self.rid,self.column,self.table)\n",
      "        print self.query1\n",
      "        self.mode = mode\n",
      "        self.dbname = dbname\n",
      "        self.getDBConnection(self.dbname)\n",
      "\n",
      "        # Create the pyConTextNLP ConTextDocument. This is the container for all the markups\n",
      "        self.document = pyConText.ConTextDocument()\n",
      "\n",
      "        self.modifiers = itemData.itemData()\n",
      "        self.targets = itemData.itemData()\n",
      "        for kb in lexical_kb:\n",
      "            self.modifiers.extend( itemData.instantiateFromCSVtoitemData(kb) )\n",
      "        for kb in domain_kb:\n",
      "            self.targets.extend( itemData.instantiateFromCSVtoitemData(kb) )\n",
      "\n",
      "\n",
      "        self.debug = debug\n",
      "        if( self.debug ):\n",
      "            print \"debug set to True\"\n",
      "            tmp = os.path.splitext(self.dbname)\n",
      "            self.debugDir = tmp[0]+\"_debug_dir\"\n",
      "            if( not os.path.exists(self.debugDir) ):\n",
      "                os.mkdir(self.debugDir)\n",
      "        else:\n",
      "            self.debugDir = ''\n",
      "            \n",
      "        self.counter = {}\n",
      "    def readReports(self):\n",
      "        self.query1 = '''SELECT %s,%s FROM %s WHERE month = \"OCT\"'''%(self.rid,\n",
      "                                                                      self.column,\n",
      "                                                                      self.table)\n",
      "        # get reports to process\n",
      "        self.cursor.execute(self.query1)\n",
      "        self.reports = self.cursor.fetchall()\n",
      "        print \"number of reports to process\",len(self.reports)\n",
      "\n",
      "    def readRules(self,fname):\n",
      "        \"\"\"read the sentence level rules\"\"\"\n",
      "        f0 = file(fname,\"r\")\n",
      "        data = f0.readlines()\n",
      "        self.class_rules = {}\n",
      "        self.category_rules = []\n",
      "        self.severity_rules = []\n",
      "        for d in data:\n",
      "            tmp = d.strip().split(\",\")\n",
      "            if( not tmp[0][0] == \"#\" ): # # comment character\n",
      "                if( tmp[0] == \"@CLASSIFICATION_RULE\" ):\n",
      "                    r = self.class_rules.get(tmp[1],{\"LABEL\":\"\",\"DEFAULT\":\"\",\"RULES\":[]})\n",
      "                    value = int(tmp[3])\n",
      "                    if(tmp[2] == 'DEFAULT'):\n",
      "                        r[\"DEFAULT\"] = value\n",
      "                    elif(tmp[2] == 'RULE'):\n",
      "                        rcs = []\n",
      "                        for rc in tmp[4:]:\n",
      "                            rcs.append(rc)\n",
      "                        r[\"RULES\"].append((value,rcs))\n",
      "                    self.class_rules[tmp[1]] = r\n",
      "                elif( tmp[0] == \"@CATEGORY_RULE\"):\n",
      "                    self.category_rules.append((tmp[1],[r for r in tmp[2:]]))\n",
      "                elif( tmp[0] == \"@SEVERITY_RULE\"):\n",
      "                    self.severity_rules.append((tmp[1],[r for r in tmp[2:]]))\n",
      "        self.classification_schema += \"\\nCLASSIFICATION_RULES\\n\"\n",
      "        for r in self.class_rules.keys():\n",
      "            self.classification_schema += \"%s %s\\n\"%(r,str(self.class_rules[r]))\n",
      "        self.classification_schema += \"\\nCATEGORY_RULES\\n\"\n",
      "        for cr in self.category_rules:\n",
      "            self.classification_schema += \"%s\\n\"%str(cr)\n",
      "        self.classification_schema += \"\\nSEVERITY_RULES\\n\"\n",
      "        for sr in self.severity_rules:\n",
      "            self.classification_schema += \"%s\\n\"%str(sr)\n",
      "\n",
      "        print self.classification_schema\n",
      "    def readSchema(self,fname):\n",
      "        \"\"\"read the schema and schema classifier functionality\"\"\"\n",
      "        fo = file(fname,\"r\")\n",
      "        self.schema = {}\n",
      "        tmp = fo.readlines()\n",
      "        for t in tmp:\n",
      "            t = t.strip().split(\",\")\n",
      "            if( t[0][0] != \"#\" ):\n",
      "                self.schema[int(t[0])]=(t[1],t[2])\n",
      "        self.classification_schema = \"\"\n",
      "        for k in self.schema.keys():\n",
      "            self.classification_schema += \"%d: %s: %s\\n\"%(k,self.schema[k][0],self.schema[k][1])\n",
      "    def _getLastRowID(self):\n",
      "        if( self.mode == 'access' ):\n",
      "            self.cursor.execute(\"\"\"SELECT @@IDENTITY AS identity\"\"\")\n",
      "            tmp = self.cursor.fetchone()\n",
      "            return tmp[0]\n",
      "        elif( self.mode == 'sqlite' ):\n",
      "            return self.cursor.lastrowid\n",
      "    def _getDBConnectionAccess(self,dbname):\n",
      "        import pyodbc\n",
      "        self.conn=pyodbc.connect(\"DRIVER={Microsoft Access Driver (*.mdb)};DBQ=%s\"%dbname)\n",
      "    def _getDBConnectionSQLite(self,dbname):\n",
      "        self.conn=sqlite.connect(dbname)\n",
      "    def getDBConnection(self,dbname):\n",
      "        if( self.mode == 'access' ):\n",
      "            self._getDBConnectionAccess(dbname)\n",
      "        else:\n",
      "            self._getDBConnectionSQLite(dbname)\n",
      "        self.cursor = self.conn.cursor()\n",
      "\n",
      "    def _createTablesAccess(self):\n",
      "        try:\n",
      "            self.cursor.execute(\"\"\"create table run_args (rowid AUTOINCREMENT,\n",
      "                                                          run_date text,\n",
      "                                                          label text,\n",
      "                                                          args text,\n",
      "                                                          PRIMARY KEY(rowid))\"\"\")\n",
      "        except:\n",
      "            print \"run_args table seems to already exist\"\n",
      "        try:\n",
      "            self.cursor.execute(\"\"\"CREATE TABLE class_schema (rowid AUTOINCREMENT,\n",
      "                                                              schema text,\n",
      "                                                              PRIMARY KEY(rowid))\"\"\")\n",
      "        except:\n",
      "            print \"class_schema table seems to exist\"\n",
      "        try:\n",
      "            self.cursor.execute(\n",
      "                    \"\"\"CREATE TABLE pyConTextNLP_results\n",
      "                       (rowid AUTOINCREMENT, \n",
      "                        report_number int, \n",
      "                        run_args int, \n",
      "                        schema int,\n",
      "\t\t\ttarget_category text,\n",
      "                        classification int, \n",
      "                        most_positive_target text,\n",
      "                        PRIMARY KEY(rowid),\n",
      "                        FOREIGN KEY (run_args) REFERENCES run_args(rowid),\n",
      "                        FOREIGN KEY (schema) REFERENCES class_schema(rowid))\"\"\")\n",
      "\n",
      "        except Exception,error:\n",
      "            print error\n",
      "            print \"pyconTextNLP_results table seems to exist\"\n",
      "        try:\n",
      "            self.cursor.execute(\n",
      "                    \"\"\"CREATE TABLE pyConTextNLP_severity\n",
      "                        (rowid AUTOINCREMENT,\n",
      "                        result int,\n",
      "                        phrase text,\n",
      "                        svalue text,\n",
      "                        units text,\n",
      "                        PRIMARY KEY(rowid),\n",
      "                        FOREIGN KEY (result) REFERENCES pyConTextNLP_results(rowid))\"\"\")\n",
      "        except Exception, error:\n",
      "            print error\n",
      "            print \"pyConTextNLP_severity table seems to exist\"\n",
      "        \n",
      "    def _createTablesSQLite(self):\n",
      "        self.cursor.execute(\"\"\"create table if not exists run_args (run_date text,\n",
      "                                                                    label text,\n",
      "                                                                    args text)\"\"\")\n",
      "        self.cursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS class_schema (schema text)\"\"\")\n",
      "\n",
      "        self.cursor.execute(\n",
      "                \"\"\"CREATE TABLE IF NOT EXISTS pyConTextNLP_results\n",
      "                    (report_number int, \n",
      "                    run_args int, \n",
      "                    schema int,\n",
      "                    target_category text,\n",
      "                    classification int, \n",
      "                    most_positive_target text,\n",
      "                    FOREIGN KEY (run_args) REFERENCES run_args(rowid),\n",
      "                    FOREIGN KEY (schema) REFERENCES class_schema(rowid))\"\"\")\n",
      "\n",
      "        self.cursor.execute(\n",
      "                \"\"\"CREATE TABLE IF NOT EXISTS pyConTextNLP_severity\n",
      "                    (result int,\n",
      "                    phrase text,\n",
      "                    svalue text,\n",
      "                    units text,\n",
      "                    FOREIGN KEY (result) REFERENCES pyConTextNLP_results(rowid))\"\"\")\n",
      "\n",
      "    def createTables(self):\n",
      "            if( self.mode == \"access\" ):\n",
      "                self._createTablesAccess()\n",
      "            elif( self.mode == \"sqlite\" ):\n",
      "                self._createTablesSQLite()\n",
      "\n",
      "    def initializeOutput(self): #rfile,lfile,dfile):\n",
      "        \"\"\"Provides run specific information for XML output file\"\"\"\n",
      "        self.outString  =u\"\"\"<?xml version=\"1.0\"?>\\n\"\"\"\n",
      "        self.outString +=u\"\"\"<args>\\n\"\"\"\n",
      "        self.outString +=u\"\"\"<pyConTextNLPVersion> %s </pyConTextNLPVersion>\\n\"\"\"%pyConTextNLP.__version__\n",
      "        self.outString +=u\"\"\"<operator> %s </operator>\\n\"\"\"%getpass.getuser()\n",
      "        self.outString +=u\"\"\"<date> %s </date>\\n\"\"\"%time.ctime()\n",
      "        self.outString +=u\"\"\"<dataFile> %s </dataFile>\\n\"\"\"%self.dbname\n",
      "        self.outString +=u\"\"\"<lexicalFile> %s </lexicalFile>\\n\"\"\"%self.lexical_kb\n",
      "        self.outString +=u\"\"\"<tableName> %s </tableName>\\n\"\"\"%self.table\n",
      "        self.outString +=u\"\"\"<columnName> %s</columnName>\\n\"\"\"%self.column\n",
      "        self.outString +=u\"\"\"<domainFile> %s </domainFile>\\n\"\"\"%self.domain_kb\n",
      "        self.outString +=u\"\"\"</args\"\"\"\n",
      "        self.run_time = time.asctime(time.localtime())\n",
      "\n",
      "# create the tables for results and run arguments\n",
      "        self.createTables()\n",
      "        self.cursor.execute(\"\"\"INSERT INTO run_args(run_date,label,args) VALUES (?,?,?)\"\"\",\n",
      "                                    (self.run_time, self.result_label, self.outString,))\n",
      "\n",
      "# get the run_args id for future REFERENCES\n",
      "        self.cursor.execute(\"\"\"SELECT rowid FROM run_args WHERE run_date = ?\"\"\",(self.run_time,))\n",
      "        self.run_args_id = self.cursor.fetchone()[0]\n",
      "\n",
      "\n",
      "\n",
      "# if the current schema is not in the table, then insert itemData\n",
      "        # See if the current schema is already in the database\n",
      "        try:\n",
      "            self.cursor.execute(\"\"\"SELECT rowid FROM class_schema WHERE schema = ?\"\"\",(self.classification_schema,))\n",
      "            self.schema_id = self.cursor.fetchone()[0]\n",
      "        except:\n",
      "            self.cursor.execute(\"\"\"INSERT INTO class_schema(schema) VALUES (?)\"\"\",(self.classification_schema,))\n",
      "            self.schema_id = self._getLastRowID()\n",
      "            #self.cursor.execute(\"\"\"SELECT rowid FROM class_schema WHERE schema = ?\"\"\",(self.classification_schema,))\n",
      "            #self.schema_id = self.cursor.fetchone()[0]\n",
      "\n",
      "\n",
      "    def closeOutput(self):\n",
      "        print \"committing changes\"\n",
      "        self.conn.commit()\n",
      "        print \"closing connection\"\n",
      "        self.conn.close()\n",
      "    def commitResults(self,rslts):#classification, positiveMarkup, documentMarkup):\n",
      "        trslts = rslts[0]\n",
      "        if( trslts ):\n",
      "            \n",
      "            for key in trslts.keys():\n",
      "                rslts = trslts[key]\n",
      "                cc = self.counter.get(key,Counter())\n",
      "                cc[rslts[0]] += 1\n",
      "                self.counter[key] = cc\n",
      "    \n",
      "                self.cursor.execute(\"\"\"INSERT INTO pyConTextNLP_results(report_number,\n",
      "                            run_args, schema, target_category, classification, most_positive_target) VALUES (?,?,?,?,?,?)\"\"\",\n",
      "                    (self.currentCase,self.run_args_id,self.schema_id, \n",
      "                     key, rslts[0], rslts[1],))\n",
      "                if( rslts[2] ): # there are severity features\n",
      "                    rslt_id = self._getLastRowID()\n",
      "                    for r in rslts[2]:\n",
      "                        self.cursor.execute(\"\"\"INSERT INTO pyConTextNLP_severity(result,phrase,svalue,units) VALUES (?,?,?,?)\"\"\",\n",
      "                                (rslt_id,r[0],r[1],r[2],))\n",
      "        else:\n",
      "            self.cursor.execute(\"\"\"INSERT INTO pyConTextNLP_results(report_number,\n",
      "                        run_args, schema, target_category, classification, most_positive_target) VALUES (?,?,?,?,?,?)\"\"\",\n",
      "                (self.currentCase,self.run_args_id,self.schema_id,\"NULL\",0,\"NULL\",)) \n",
      "    def analyzeReport(self, report ):\n",
      "        \"\"\"\n",
      "        given an individual radiology report, creates a pyConTextGraph\n",
      "        object that contains the context markup\n",
      "        report: a text string containing the radiology reports\n",
      "        \"\"\"\n",
      "        context = self.document\n",
      "        targets = self.targets\n",
      "        modifiers = self.modifiers\n",
      "        splitter = helpers.sentenceSplitter()\n",
      "# alternatively you can skip the default exceptions and add your own\n",
      "#       splitter = helpers.sentenceSpliter(useDefaults = False)\n",
      "        #splitter.addExceptionTerms(\"Dr.\",\"Mr.\",\"Mrs.\",M.D.\",\"R.N.\",\"L.P.N.\",addCaseVariants=True)\n",
      "        splitter.addExceptionTerms(\"Ms.\",\"D.O.\",addCaseVariants=True)\n",
      "        splitter.deleteExceptionTerms(\"A.B.\",\"B.S.\",deleteCaseVariants=True)\n",
      "        sentences = splitter.splitSentences(report)\n",
      "        count = 0\n",
      "        for s in sentences:\n",
      "            #print s\n",
      "            markup = pyConText.ConTextMarkup()\n",
      "            #markup.toggleVerbose()\n",
      "            markup.setRawText(s)\n",
      "            markup.cleanText()\n",
      "            markup.markItems(modifiers, mode=\"modifier\")\n",
      "            markup.markItems(targets, mode=\"target\")\n",
      "            #raw_input('marked targets and modifiers')\n",
      "            #print \"markup before pruning\"\n",
      "            #print markup.getXML()\n",
      "            markup.pruneMarks()\n",
      "            markup.dropMarks('Exclusion')\n",
      "            # apply modifiers to any targets within the modifiers scope\n",
      "            markup.applyModifiers()\n",
      "\n",
      "            markup.pruneSelfModifyingRelationships()\n",
      "            #context.pruneModifierRelationships()\n",
      "            #context.dropInactiveModifiers()\n",
      "            # add markup to context document\n",
      "            #print markup\n",
      "            context.addMarkup(markup)\n",
      "            count += 1\n",
      "        context.computeDocumentGraph()\n",
      "        #print \"*\"*42\n",
      "        #print context.getXML(currentGraph=False)\n",
      "        #print \"*\"*42\n",
      "\n",
      "    def processReports(self):\n",
      "        \"\"\"For the selected reports (training or testing) in the database,\n",
      "        process each report \n",
      "        \"\"\"\n",
      "        for r in self.reports:\n",
      "            #if r[0] in [275,304,305]:\n",
      "            self.document = pyConText.ConTextDocument()\n",
      "            #try:\n",
      "            if(True):\n",
      "                if( r[0] % 1000 == 0 ):\n",
      "                    print r[0],\n",
      "                self.currentCase = r[0]\n",
      "                \n",
      "                self.currentText = r[1].lower()\n",
      "                #print \"CurrentCase:\",self.currentCase\n",
      "                #print r[1].lower()\n",
      "                self.analyzeReport(self.currentText)\n",
      "                if( self.debug ):\n",
      "                    self.writeDebugInfo()\n",
      "                rslts = self.classifyDocumentTargets()\n",
      "                self.commitResults(rslts)\n",
      "                self.conn.commit()\n",
      "            #except Exception, error:\n",
      "                #print \"error\",error\n",
      "                #print \"failed to process report\",r\n",
      "                    \n",
      "    def writeDebugInfo(self ):\n",
      "        \"\"\"write out an XML version of the current ConTextDocument markup for debugging purposes\"\"\"\n",
      "\n",
      "        fout = open(os.path.join(self.debugDir,\"%04d_debug.xml\"%self.currentCase),\"w\")\n",
      "        txt = self.document.getXML()\n",
      "        try:\n",
      "            xml = minidom.parseString(txt)\n",
      "            fout.write(xml.toprettyxml(encoding=self.document.getUnicodeEncoding()))\n",
      "        except Exception, error:\n",
      "            print \"could not prettify debug output\", error\n",
      "            fout.write(txt)\n",
      "        fout.close()\n",
      "\n",
      "    def classifyResult(self, docr):\n",
      "        \"\"\"given results in doc_rslts compare to classification_schema and return score.\n",
      "        Takes a three-tuple of boolean values for Disease State Positive, Disease State Certain, Disease State Acute\"\"\"\n",
      "        return assignSchema(docr,self.schema)\n",
      "    def classifyDocumentTargets(self):\n",
      "        \"\"\"\n",
      "        Look at the targets and their modifiers to get an overall classification for the document_markup\n",
      "        \"\"\"\n",
      "        #print \"in classifyDocumentTargets\"\n",
      "        rslts = {} #[0,'']\n",
      "        \n",
      "        qualityInducedUncertainty = False\n",
      "        document_markup = self.document.getXML()\n",
      "        g = self.document.getDocumentGraph()\n",
      "        #nx.draw_networkx(g,pos=nx.spring_layout(g))\n",
      "        targets = [n[0] for n in g.nodes(data = True) if n[1].get(\"category\",\"\") == 'target']\n",
      "\n",
      "      \n",
      "        if( targets ):\n",
      "            neg_filters = [\"definite_negated_existence\",\"probable_negated_existence\"]\n",
      "            for t in targets:\n",
      "                severityValues = []\n",
      "                current_rslts = {}\n",
      "                currentCategory = t.getCategory()\n",
      "                if( not t.isA([\"QUALITY_FEATURE\",\"ARTIFACT\"]) ):\n",
      "                    for rk in self.class_rules:\n",
      "                        current_rslts[rk] = genericClassifier(g,t,self.class_rules[rk])\n",
      "                    for cr in self.category_rules:\n",
      "                        anatomyRecategorize(g,t,cr)\n",
      "                    for sv in self.severity_rules:\n",
      "                        severity = getSeverity(g,t,sv)\n",
      "                        severityValues.extend(severity)\n",
      "                    currentCategory = t.categoryString()\n",
      "                    #print currentCategory\n",
      "                    # now need to compare current_rslts to rslts to select most Positive\n",
      "                    docr = self.classifyResult(current_rslts)\n",
      "                    trslts = rslts.get(currentCategory,[-1,'',[]])\n",
      "                    if( trslts[0] < docr ):\n",
      "                        trslts = [docr,t.getXML(),severityValues]\n",
      "                    rslts[currentCategory] = trslts \n",
      "                else:\n",
      "                    if( t.isA('QUALITY_FEATURE')):\n",
      "                        qualityInducedUncertainty = True\n",
      "                    else:\n",
      "                        if( not modifies(g,t,neg_filters) ):# non-negated artifacts\n",
      "                            qualityInducedUncertainty = True\n",
      "        # Need to fix this to reflect new representation\n",
      "   #     if( qualityInducedUncertainty ): # if exam quality is described as limited, then always set certainty to be False\n",
      "            #for key in rslts.keys():\n",
      "                #rslts[key][1] = 0\n",
      "\n",
      "        return rslts,document_markup\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getOptions():\n",
      "    \"\"\"Generates arguments for specifying database and other parameters\"\"\"\n",
      "    options = {}\n",
      "    options['dbname'] = os.path.join(DATADIR2,\"criticalFindingsAll.sqlite\")\n",
      "    # specify database mode. I like using SQLite, but the program also supports MS Access with the value 'access'\n",
      "    options['mode'] = 'sqlite' \n",
      "    # Specify\n",
      "    options['lexical_kb'] = [os.path.join(DATADIR,\"Amil/KB/lexical_kb_04292013.tsv\"), \n",
      "                             os.path.join(DATADIR,\"Amil/KB/criticalfinder_generalized_modifiers.tsv\")]\n",
      "    options['domain_kb'] = [os.path.join(DATADIR,\"Amil/KB/domain_kb_test.tsv\")]\n",
      "    options['table'] = 'reports'#'table in database to select data from'\n",
      "    options['id'] = 'rowid' #'column in table to select identifier from'\n",
      "    options['report_text'] = 'impression' #'column in table to select report text from'\n",
      "    options[\"result_label\"] = \"cf_marta1\" # \"result label for inserting into database\"\n",
      "    options[\"debug\"] =True # \"set debug to true for verbose output to text files\")\n",
      "    options[\"schema\"] = os.path.join(DATADIR,\"Amil/amilCode/trunk/schema2.csv\")#\"file specifying schema\"\n",
      "    options[\"rules\"] = \"/Users/brian/Dropbox/NLP/Amil/KB/06242013/classificationRules3.csv\" # \"file specifying sentence level rules\")\n",
      "    return options\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "\n",
      "def outputCounter(schema,cntr):\n",
      "    schemaKeys = schema.keys()\n",
      "    schemaKeys.sort()\n",
      "    s = \"\"\"<table>\\n\"\"\"\n",
      "    ckeys = cntr.keys()\n",
      "    ckeys.sort()\n",
      "    s += \"\"\"<tr><th>Category</th>\"\"\"\n",
      "    for sk in schemaKeys:\n",
      "        s += \"\"\"<th>%s</th>\"\"\"%schema[sk][0]\n",
      "    s += \"\"\"</tr>\\n\"\"\"\n",
      "    for ck in ckeys:\n",
      "        cck = cntr[ck]\n",
      "        s += \"\"\"<tr>\"\"\"\n",
      "        s += \"\"\"<td>%s</td>\"\"\"%ck\n",
      "        \n",
      "        for i in schemaKeys:\n",
      "            s += \"\"\"<td>%s</td>\"\"\"%cck[i]\n",
      "        s += \"\"\"</tr>\\n\"\"\"\n",
      "    s += \"\"\"</table>\\n\"\"\"\n",
      "    return HTML(s)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "options  = getOptions()\n",
      "pec = criticalFinder(schema=options[\"schema\"],rules=options[\"rules\"],rid=options[\"id\"],column=options[\"report_text\"],\n",
      "                     table=options[\"table\"],result_label=options[\"result_label\"],\n",
      "                     mode=options[\"mode\"],dbname=options[\"dbname\"],lexical_kb=options[\"lexical_kb\"],domain_kb=options[\"domain_kb\"],\n",
      "                     debug=options[\"debug\"])\n",
      "print \"created criticalFinder instance\"\n",
      "pec.initializeOutput()\n",
      "pec.readReports()\n",
      "#pec.reports = [r for r in pec.reports if r[0] in report_rowids]\n",
      "print \"number of reports to process\",len(pec.reports)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pec.processReports()\n",
      "pec.closeOutput()\n",
      "h = outputCounter(pec.schema,pec.counter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conn = sqlite.connect(os.path.join(DATADIR2,\"criticalFindingsAll.sqlite\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cursor = conn.cursor()\n",
      "cursor.execute(\"\"\"SELECT * from pyConTextNLP_results WHERE target_category = \"heart_anatomy_infarct_brain_anatomy_infarct\" AND run_args = 10 \"\"\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = cursor.fetchall()\n",
      "print(len(data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "codingKey = {1:\"AMBIVALENT\",\n",
      "2: \"Negative/Certain/Acute\",\n",
      "3: \"Negative/Uncertain/Chronic\",\n",
      "4: \"Positive/Uncertain/Chronic\",\n",
      "5: \"Positive/Certain/Chronic\",\n",
      "6: \"Negative/Uncertain/Acute\",\n",
      "7: \"Positive/Uncertain/Acute\",\n",
      "8: \"Positive/Certain/Acute\"}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "report_rowids = [d[0] for d in data]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for d in data:\n",
      "    cursor.execute(\"\"\"SELECT accession,impression FROM reports WHERE rowid = ?\"\"\",(d[0],))\n",
      "    impression = cursor.fetchone()\n",
      "    print \"report id= %s; accession = %s\"%(d[0], impression[0])\n",
      "    print\n",
      "    print impression[1]\n",
      "    print\n",
      "    print \"-\"*42\n",
      "    print \"classification=%s\"%codingKey[d[4]]\n",
      "    print \"-\"*42\n",
      "    print \"*\"*42\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(pec.modifiers)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pec.modifiers[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for m in pec.modifiers:\n",
      "    if 'heart_anatomy' in m.getCategory():\n",
      "        print m"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regexs = [m.getRE() for m in pec.modifiers if m.getLiteral() == \"right ventricular wall\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(regexs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "txt = \"new abnormal flair hyperintensity within the adjacent subcortical white matter compared to 2011 likely represents interval peri-infarct ischemia\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regexs[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pyConText.compiledRegExprs.has_key(u'right ventricular wall')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rvw = pyConText.compiledRegExprs[u'right ventricular wall']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rvw.findall(txt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(pyConText)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}